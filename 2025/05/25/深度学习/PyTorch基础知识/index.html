<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>学习 Pytorch 基础知识 | 代码手记</title><meta name="author" content="zyw9825"><meta name="copyright" content="zyw9825"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="PyTorch 简介 Torch 是一个与 Numpy 类似的张量（Tensor）操作库，与 Numpy 不同的是 Torch 对 GPU 支持的很好 PyTorch 是一个基于 Torch 的 Python 开源机器学习库，用于自然语言处理等应用程序。它主要由 Facebook 的人工智能研究小组开发。 PyTorch 是一个 Python 包，提供两个高级功能：  具有强大的 GPU 加速的张">
<meta property="og:type" content="article">
<meta property="og:title" content="学习 Pytorch 基础知识">
<meta property="og:url" content="https://zyw9825.github.io/2025/05/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PyTorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/index.html">
<meta property="og:site_name" content="代码手记">
<meta property="og:description" content="PyTorch 简介 Torch 是一个与 Numpy 类似的张量（Tensor）操作库，与 Numpy 不同的是 Torch 对 GPU 支持的很好 PyTorch 是一个基于 Torch 的 Python 开源机器学习库，用于自然语言处理等应用程序。它主要由 Facebook 的人工智能研究小组开发。 PyTorch 是一个 Python 包，提供两个高级功能：  具有强大的 GPU 加速的张">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog-img-save.oss-cn-chengdu.aliyuncs.com/bg/BG/wukong.png">
<meta property="article:published_time" content="2025-05-24T16:33:10.260Z">
<meta property="article:modified_time" content="2025-05-24T16:25:06.000Z">
<meta property="article:author" content="zyw9825">
<meta property="article:tag" content="PyTroch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog-img-save.oss-cn-chengdu.aliyuncs.com/bg/BG/wukong.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "学习 Pytorch 基础知识",
  "url": "https://zyw9825.github.io/2025/05/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PyTorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/",
  "image": "https://blog-img-save.oss-cn-chengdu.aliyuncs.com/bg/BG/wukong.png",
  "datePublished": "2025-05-24T16:33:10.260Z",
  "dateModified": "2025-05-24T16:25:06.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "zyw9825",
      "url": "https://zyw9825.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="https://blog-img-save.oss-cn-chengdu.aliyuncs.com/book.svg"><link rel="canonical" href="https://zyw9825.github.io/2025/05/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PyTorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?08ead5bb7d005bd4116f0ec90195f830";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '学习 Pytorch 基础知识',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://blog-img-save.oss-cn-chengdu.aliyuncs.com/bg/BG/wukong.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-comments"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment"></i><span> 留言板</span></a></div></div></div></div><div class="post type-note" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://blog-img-save.oss-cn-chengdu.aliyuncs.com/bg/BG/11ps.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">代码手记</span></a><a class="nav-page-title" href="/"><span class="site-name">学习 Pytorch 基础知识</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-comments"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">学习 Pytorch 基础知识</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-05-24T16:33:10.260Z" title="发表于 2025-05-25 00:33:10">2025-05-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-24T16:25:06.000Z" title="更新于 2025-05-25 00:25:06">2025-05-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PyTorch/">PyTorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="PyTorch-简介">PyTorch 简介</h1>
<p>Torch 是一个与 Numpy 类似的张量（Tensor）操作库，与 Numpy 不同的是 Torch 对 GPU 支持的很好</p>
<p>PyTorch 是一个基于 Torch 的 Python 开源机器学习库，用于自然语言处理等应用程序。它主要由 Facebook 的人工智能研究小组开发。</p>
<p>PyTorch 是一个 Python 包，提供两个高级功能：</p>
<ul>
<li>具有强大的 GPU 加速的张量计算</li>
<li>包含自动求导系统的的深度神经网络</li>
</ul>
<h1 id="学习-PyTorch-基础知识"><a target="_blank" rel="noopener" href="https://docs.pytorch.org/tutorials/beginner/basics/intro.html">学习 PyTorch 基础知识</a></h1>
<p>大多数机器学习工作流程都涉及处理数据、创建模型、优化模型参数以及保存已训练的模型。</p>
<p>本教程将向您介绍使用 PyTorch 实现的完整机器学习工作流程，并提供相关链接，帮助您进一步了解每个概念。</p>
<p>我们将使用 FashionMNIST 数据集来训练一个神经网络，用于图片分类。</p>
<h2 id="快速入门">快速入门</h2>
<h3 id="处理数据">处理数据</h3>
<p>PyTorch 提供了两个用于处理数据的基本工具：</p>
<ul>
<li><code>torch.utils.data.DataLoader</code></li>
<li><code>torch.utils.data.Dataset</code></li>
</ul>
<p><code>Dataset</code> 存储样本及其对应的标签，而 <code>DataLoader</code> 则为 <code>Dataset</code> 提供了一个可迭代的对象，支持自动分批、采样、打乱和多进程加载数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br></pre></td></tr></table></figure>
<p>PyTorch 提供了多个面向特定领域的库，例如 TorchText、TorchVision 和 TorchAudio，它们都包含了各种数据集。在本教程中，我们将使用 TorchVision 提供的数据集。</p>
<p><code>torchvision.datasets</code> 模块包含许多现实世界视觉数据集，如 CIFAR、COCO（完整列表见<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/datasets.html">这里</a> ）。本教程中我们使用的是 FashionMNIST 数据集。每个 TorchVision 数据集都包含两个参数：<code>transform</code> 和 <code>target_transform</code>，分别用于修改样本和标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载训练数据</span></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor(),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载测试数据</span></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor(),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>下载过程略（进度条显示下载状态）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=batch_size)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> test_dataloader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Shape of X [N, C, H, W]: <span class="subst">&#123;X.shape&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Shape of y: <span class="subst">&#123;y.shape&#125;</span> <span class="subst">&#123;y.dtype&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Shape of X [N, C, H, W]: torch.Size([<span class="number">64</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">Shape of y: torch.Size([<span class="number">64</span>]) torch.int64</span><br></pre></td></tr></table></figure>
<h3 id="创建模型">创建模型</h3>
<p>要在 PyTorch 中定义神经网络，我们需要创建一个继承自 <code>nn.Module</code> 的类。我们在 <code>__init__</code> 函数中定义网络层，并在 <code>forward</code> 函数中指定数据如何在网络中流动。</p>
<blockquote>
<p>为了加速神经网络操作，我们可以将其移动到加速器上，比如 CUDA、MPS、MTIA 或 XPU。如果当前有可用的加速器，则使用它；否则使用 CPU。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">device = torch.accelerator.current_accelerator().<span class="built_in">type</span> <span class="keyword">if</span> torch.accelerator.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Using <span class="subst">&#123;device&#125;</span> device&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()</span><br><span class="line">        <span class="variable language_">self</span>.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x)</span><br><span class="line">        logits = <span class="variable language_">self</span>.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Using cuda device</span><br><span class="line">NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=<span class="number">1</span>, end_dim=-<span class="number">1</span>)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">784</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): ReLU()</span><br><span class="line">    (<span class="number">2</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">3</span>): ReLU()</span><br><span class="line">    (<span class="number">4</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="优化模型参数">优化模型参数</h3>
<p>要训练模型，我们需要一个损失函数和一个优化器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure>
<p>在一个完整的训练循环中，模型会对训练数据集进行预测（按批次输入），并通过反向传播来调整模型参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">dataloader, model, loss_fn, optimizer</span>):</span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算预测误差</span></span><br><span class="line">        pred = model(X)</span><br><span class="line">        loss = loss_fn(pred, y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            loss, current = loss.item(), (batch + <span class="number">1</span>) * <span class="built_in">len</span>(X)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;loss: <span class="subst">&#123;loss:&gt;7f&#125;</span>  [<span class="subst">&#123;current:&gt;5d&#125;</span>/<span class="subst">&#123;size:&gt;5d&#125;</span>]&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>我们还会检查模型在测试数据集上的表现，以确保它正在学习：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">dataloader, model, loss_fn</span>):</span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">    num_batches = <span class="built_in">len</span>(dataloader)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    test_loss, correct = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> dataloader:</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            pred = model(X)</span><br><span class="line">            test_loss += loss_fn(pred, y).item()</span><br><span class="line">            correct += (pred.argmax(<span class="number">1</span>) == y).<span class="built_in">type</span>(torch.<span class="built_in">float</span>).<span class="built_in">sum</span>().item()</span><br><span class="line">    test_loss /= num_batches</span><br><span class="line">    correct /= size</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Test Error: \n Accuracy: <span class="subst">&#123;(<span class="number">100</span>*correct):&gt;<span class="number">0.1</span>f&#125;</span>%, Avg loss: <span class="subst">&#123;test_loss:&gt;8f&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>训练过程会在多个迭代周期（epochs）中进行。在每个 epoch 中，模型会学习参数以做出更好的预测。我们在每个 epoch 后打印模型的准确率和损失；理想情况下，随着训练的进行，准确率应该上升，损失应该下降。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;t+<span class="number">1</span>&#125;</span>\n-------------------------------&quot;</span>)</span><br><span class="line">    train(train_dataloader, model, loss_fn, optimizer)</span><br><span class="line">    test(test_dataloader, model, loss_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Done!&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出示例（训练日志略）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1</span><br><span class="line">-------------------------------</span><br><span class="line">loss: 2.296921  [   64/60000]</span><br><span class="line">...</span><br><span class="line">Test Error:</span><br><span class="line"> Accuracy: 47.5%, Avg loss: 2.146411</span><br><span class="line"></span><br><span class="line">Epoch 2</span><br><span class="line">-------------------------------</span><br><span class="line">...</span><br><span class="line">Test Error:</span><br><span class="line"> Accuracy: 59.2%, Avg loss: 1.874406</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">Epoch 5</span><br><span class="line">-------------------------------</span><br><span class="line">...</span><br><span class="line">Test Error:</span><br><span class="line"> Accuracy: 64.7%, Avg loss: 1.082125</span><br><span class="line"></span><br><span class="line">Done!</span><br></pre></td></tr></table></figure>
<h3 id="保存模型">保存模型</h3>
<p>保存模型的一种常见方法是序列化其内部状态字典（包含模型参数）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&quot;model.pth&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Saved PyTorch Model State to model.pth&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="加载模型">加载模型</h3>
<p>加载模型的过程包括重新创建模型结构并从文件中加载状态字典。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&quot;model.pth&quot;</span>, weights_only=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;All keys matched successfully&gt;</span><br></pre></td></tr></table></figure>
<p>现在这个模型可以用来进行预测了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">classes = [</span><br><span class="line">    <span class="string">&quot;T-shirt/top&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Trouser&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Pullover&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Dress&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Coat&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Sandal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Shirt&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Sneaker&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Bag&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Ankle boot&quot;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">x, y = test_data[<span class="number">0</span>][<span class="number">0</span>], test_data[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    pred = model(x)</span><br><span class="line">    predicted, actual = classes[pred[<span class="number">0</span>].argmax(<span class="number">0</span>)], classes[y]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Predicted: &quot;<span class="subst">&#123;predicted&#125;</span>&quot;, Actual: &quot;<span class="subst">&#123;actual&#125;</span>&quot;&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Predicted: <span class="string">&quot;Ankle boot&quot;</span>, Actual: <span class="string">&quot;Ankle boot&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="张量">张量</h2>
<p>张量（Tensors）是一种专门的数据结构，与数组和矩阵非常相似。在 PyTorch 中，我们使用张量来编码模型的输入和输出，以及模型的参数。</p>
<p>张量类似于 NumPy 的 <code>ndarrays</code>，但不同之处在于张量可以在 GPU 或其他硬件加速器上运行。事实上，张量和 NumPy 数组通常可以共享底层内存，从而避免了数据复制（参见“<a target="_blank" rel="noopener" href="https://docs.pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label">与 NumPy 的桥接</a>”部分）。张量还针对自动微分进行了优化（我们将在后面的 Autograd 部分详细介绍）。如果你熟悉 <code>ndarrays</code>，那么你会很快适应 Tensor API。如果不熟悉，也请继续阅读！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<h3 id="初始化张量">初始化张量</h3>
<p>张量可以通过多种方式进行初始化。请看以下示例：</p>
<h4 id="直接从数据中创建">直接从数据中创建</h4>
<p>张量可以直接从数据中创建。数据类型会自动推断。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>],[<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br></pre></td></tr></table></figure>
<h4 id="从-NumPy-数组创建">从 NumPy 数组创建</h4>
<p>张量可以从 NumPy 数组创建（反之亦然——参见“<a target="_blank" rel="noopener" href="https://docs.pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label">与 NumPy 的桥接</a>”）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np_array = np.array(data)</span><br><span class="line">x_np = torch.from_numpy(np_array)</span><br></pre></td></tr></table></figure>
<h4 id="从另一个张量创建">从另一个张量创建</h4>
<p>新张量将保留原张量的属性（形状、数据类型），除非显式覆盖。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_ones = torch.ones_like(x_data) <span class="comment"># 保留 x_data 的属性</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;x_ones&#125;</span> \n&quot;</span>)</span><br><span class="line"></span><br><span class="line">x_rand = torch.rand_like(x_data, dtype=torch.<span class="built_in">float</span>) <span class="comment"># 覆盖 x_data 的数据类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;x_rand&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Ones Tensor:</span><br><span class="line"> tensor([[1, 1],</span><br><span class="line">        [1, 1]])</span><br><span class="line"></span><br><span class="line">Random Tensor:</span><br><span class="line"> tensor([[0.5838, 0.6899],</span><br><span class="line">        [0.2495, 0.8622]])</span><br></pre></td></tr></table></figure>
<h4 id="使用随机值或常数值创建">使用随机值或常数值创建</h4>
<p><code>shape</code> 是一个表示张量维度的元组。在下面的函数中，它决定了输出张量的维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">shape = (<span class="number">2</span>,<span class="number">3</span>,)</span><br><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;rand_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;ones_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Zeros Tensor: \n <span class="subst">&#123;zeros_tensor&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Random Tensor:</span><br><span class="line"> tensor([[0.9309, 0.1657, 0.7971],</span><br><span class="line">        [0.2916, 0.7497, 0.3308]])</span><br><span class="line"></span><br><span class="line">Ones Tensor:</span><br><span class="line"> tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]])</span><br><span class="line"></span><br><span class="line">Zeros Tensor:</span><br><span class="line"> tensor([[0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br></pre></td></tr></table></figure>
<h3 id="张量的属性">张量的属性</h3>
<p>张量的属性描述了其形状、数据类型以及存储设备。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;张量的形状: <span class="subst">&#123;tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;张量的数据类型: <span class="subst">&#123;tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;张量所在的设备: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">张量的形状: torch.Size([3, 4])</span><br><span class="line">张量的数据类型: torch.float32</span><br><span class="line">张量所在的设备: cpu</span><br></pre></td></tr></table></figure>
<h3 id="对张量的操作">对张量的操作</h3>
<p>PyTorch 提供了超过 1200 种张量操作，包括算术运算、线性代数、矩阵操作（转置、索引、切片）、采样等，详见<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html">官方文档</a>。</p>
<p>这些操作都可以在 CPU 或加速器（如 CUDA、MPS、MTIA 或 XPU）上运行。如果您正在使用 Colab，请前往 <strong>Runtime &gt; Change runtime type &gt; GPU</strong> 分配加速器。</p>
<p>默认情况下，张量是在 CPU 上创建的。我们需要使用 <code>.to</code> 方法将张量显式移动到加速器上（前提是加速器可用）。请注意，在设备之间复制大型张量可能会消耗大量时间和内存！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果有可用加速器，则将张量移至当前加速器</span></span><br><span class="line"><span class="keyword">if</span> torch.accelerator.is_available():</span><br><span class="line">    tensor = tensor.to(torch.accelerator.current_accelerator())</span><br></pre></td></tr></table></figure>
<p>下面尝试一些操作。如果您熟悉 NumPy 的 API，那么 Tensor API 将非常容易上手。</p>
<h4 id="标准的-NumPy-风格索引和切片">标准的 NumPy 风格索引和切片</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.ones(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;第一行: <span class="subst">&#123;tensor[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;第一列: <span class="subst">&#123;tensor[:, <span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最后一列: <span class="subst">&#123;tensor[..., -<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br><span class="line">tensor[:,<span class="number">1</span>] = <span class="number">0</span>    <span class="comment"># 第二列全部赋值0</span></span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">第一行: tensor([1., 1., 1., 1.])</span><br><span class="line">第一列: tensor([1., 1., 1., 1.])</span><br><span class="line">最后一列: tensor([1., 1., 1., 1.])</span><br><span class="line">tensor([[1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.]])</span><br></pre></td></tr></table></figure>
<h4 id="拼接张量">拼接张量</h4>
<p>你可以使用 <code>torch.cat</code> 沿指定维度拼接一系列张量。另请参见 <code>torch.stack</code>，这是另一种略有不同的张量拼接操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])</span><br></pre></td></tr></table></figure>
<h4 id="算术运算">算术运算</h4>
<table>
<thead>
<tr>
<th>操作类型</th>
<th>符号或函数</th>
<th>数学含义</th>
<th>输出形状</th>
<th>是否改变维度</th>
</tr>
</thead>
<tbody>
<tr>
<td>矩阵乘法</td>
<td><code>@</code>, <code>torch.matmul()</code></td>
<td>线性代数中的矩阵乘法</td>
<td><code>(n, n)</code></td>
<td>可能改变</td>
</tr>
<tr>
<td>逐元素乘法</td>
<td><code>*</code>, <code>torch.mul()</code></td>
<td>对应位置元素相乘</td>
<td>与输入相同</td>
<td>不改变</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算两个张量之间的矩阵乘法。y1, y2, y3 的值相同。</span></span><br><span class="line"><span class="comment"># ``tensor.T`` 返回张量的转置</span></span><br><span class="line">y1 = tensor @ tensor.T</span><br><span class="line">y2 = tensor.matmul(tensor.T)</span><br><span class="line"></span><br><span class="line">y3 = torch.rand_like(y1)</span><br><span class="line">torch.matmul(tensor, tensor.T, out=y3)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算逐元素乘积（即两个张量中对应位置的元素相乘）。z1, z2, z3 的值相同</span></span><br><span class="line">z1 = tensor * tensor</span><br><span class="line">z2 = tensor.mul(tensor)</span><br><span class="line"></span><br><span class="line">z3 = torch.rand_like(tensor)</span><br><span class="line">torch.mul(tensor, tensor, out=z3)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.]])</span><br></pre></td></tr></table></figure>
<h4 id="单个元素张量">单个元素张量</h4>
<p>如果你有一个单个元素的张量，例如通过聚合所有张量值为一个值，你可以使用 <code>item()</code> 将其转换为 Python 数值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">agg = tensor.<span class="built_in">sum</span>()</span><br><span class="line">agg_item = agg.item()</span><br><span class="line"><span class="built_in">print</span>(agg_item, <span class="built_in">type</span>(agg_item))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">12.0 &lt;class &#x27;float&#x27;&gt;</span><br></pre></td></tr></table></figure>
<h4 id="原地操作（In-place-Operations）">原地操作（In-place Operations）</h4>
<p>将结果直接保存在操作数中的操作称为<strong>原地操作</strong>。它们以 <code>_</code> 后缀标识。例如：<code>x.copy_(y)</code>、<code>x.t_()</code> 会改变 <code>x</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;tensor&#125;</span> \n&quot;</span>)</span><br><span class="line">tensor.add_(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.]])</span><br><span class="line"></span><br><span class="line">tensor([[6., 5., 6., 6.],</span><br><span class="line">        [6., 5., 6., 6.],</span><br><span class="line">        [6., 5., 6., 6.],</span><br><span class="line">        [6., 5., 6., 6.]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：<br>
原地操作节省了一些内存，但在计算导数时可能会因历史记录的丢失而出现问题。因此，不推荐使用。</p>
</blockquote>
<h3 id="与-NumPy-的桥接">与 NumPy 的桥接</h3>
<p>位于 CPU 上的张量和 NumPy 数组可以共享底层内存位置，并且更改其中一个会影响另一个。</p>
<h4 id="张量转-NumPy-数组">张量转 NumPy 数组</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t = torch.ones(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line">n = t.numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([1., 1., 1., 1., 1.])</span><br><span class="line">n: [1. 1. 1. 1. 1.]</span><br></pre></td></tr></table></figure>
<p>对张量的修改也会反映在 NumPy 数组中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t.add_(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([2., 2., 2., 2., 2.])</span><br><span class="line">n: [2. 2. 2. 2. 2.]</span><br></pre></td></tr></table></figure>
<h4 id="NumPy-数组转张量">NumPy 数组转张量</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n = np.ones(<span class="number">5</span>)</span><br><span class="line">t = torch.from_numpy(n)</span><br></pre></td></tr></table></figure>
<p>对 NumPy 数组的修改也会反映在张量中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.add(n, <span class="number">1</span>, out=n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span><br><span class="line">n: [2. 2. 2. 2. 2.]</span><br></pre></td></tr></table></figure>
<h2 id="数据集与数据加载器">数据集与数据加载器</h2>
<p>处理数据样本的代码可能会变得杂乱且难以维护。理想情况下，我们希望将数据集代码与模型训练代码解耦，以提高可读性和模块化。PyTorch 提供了两个数据基本组件：<code>torch.utils.data.DataLoader</code> 和 <code>torch.utils.data.Dataset</code>，它们允许你使用预加载的数据集以及自定义的数据。</p>
<ul>
<li><code>Dataset</code> 存储样本及其对应的标签。</li>
<li><code>DataLoader</code> 为 <code>Dataset</code> 提供了一个可迭代对象，使我们可以方便地访问这些样本。</li>
</ul>
<p>PyTorch 各个领域的库提供了许多预加载的数据集（例如 <a target="_blank" rel="noopener" href="https://research.zalando.com/project/fashion_mnist/fashion_mnist/">FashionMNIST</a>），它们继承自 <code>torch.utils.data.Dataset</code> 并实现了特定于该类数据的功能。你可以用它们来快速构建和测试你的模型。你可以在以下链接中找到它们：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/datasets.html">图像数据集</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/text/stable/datasets.html">文本数据集</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/audio/stable/datasets.html">音频数据集</a></li>
</ul>
<h3 id="加载数据集">加载数据集</h3>
<p>下面是一个如何从 TorchVision 加载 <strong><a target="_blank" rel="noopener" href="https://research.zalando.com/project/fashion_mnist/fashion_mnist/">Fashion-MNIST</a></strong> 数据集的例子。Fashion-MNIST 是一个包含 Zalando 商品图片的数据集，共有 60,000 张训练图片和 10,000 张测试图片。每张图片是 28×28 的灰度图，并对应一个从 10 个类别中选择的标签。</p>
<p>我们使用如下参数加载 <code>FashionMNIST</code> 数据集：</p>
<ul>
<li><code>root</code>：训练/测试数据存储路径；</li>
<li><code>train</code>：指定是训练集还是测试集；</li>
<li><code>download=True</code>：如果数据不在 <code>root</code> 路径下，则从网络下载；</li>
<li><code>transform</code> 和 <code>target_transform</code>：分别用于特征和标签的转换。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>下载过程中会显示进度条，如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0%|          | 0.00/26.4M [00:00&lt;?, ?B/s]</span><br><span class="line"> ...</span><br><span class="line">100%|##########| 26.4M/26.4M [00:01&lt;00:00, 19.3MB/s]</span><br></pre></td></tr></table></figure>
<h3 id="遍历并可视化数据集">遍历并可视化数据集</h3>
<p>我们可以像访问列表一样手动索引 <code>Dataset</code>：<code>training_data[index]</code>。</p>
<p>这个示例使用 <code>matplotlib</code> 来可视化训练集中的一些样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">labels_map = &#123;</span><br><span class="line">    <span class="number">0</span>: <span class="string">&quot;T-Shirt&quot;</span>,</span><br><span class="line">    <span class="number">1</span>: <span class="string">&quot;Trouser&quot;</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;Pullover&quot;</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;Dress&quot;</span>,</span><br><span class="line">    <span class="number">4</span>: <span class="string">&quot;Coat&quot;</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;Sandal&quot;</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;Shirt&quot;</span>,</span><br><span class="line">    <span class="number">7</span>: <span class="string">&quot;Sneaker&quot;</span>,</span><br><span class="line">    <span class="number">8</span>: <span class="string">&quot;Bag&quot;</span>,</span><br><span class="line">    <span class="number">9</span>: <span class="string">&quot;Ankle Boot&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line">figure = plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">cols, rows = <span class="number">3</span>, <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, cols * rows + <span class="number">1</span>):</span><br><span class="line">    sample_idx = torch.randint(<span class="built_in">len</span>(training_data), size=(<span class="number">1</span>,)).item()    <span class="comment"># 随机样本索引</span></span><br><span class="line">    img, label = training_data[sample_idx]    <span class="comment"># 获取样本图片和标签</span></span><br><span class="line">    figure.add_subplot(rows, cols, i)</span><br><span class="line">    plt.title(labels_map[label])</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.imshow(img.squeeze(), cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://blog-img-save.oss-cn-chengdu.aliyuncs.com/img/file-20250524194109379.png" alt="|500"></p>
<h3 id="创建自定义数据集">创建自定义数据集</h3>
<p>一个自定义的 <code>Dataset</code> 类必须实现三个函数：</p>
<ul>
<li><code>__init__</code></li>
<li><code>__len__</code></li>
<li><code>__getitem__</code></li>
</ul>
<p>下面是一个示例实现。</p>
<p>假设你的图像文件保存在目录 <code>img_dir</code> 中，而标签信息保存在一个 CSV 文件 <code>annotations_file</code> 中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torchvision.io <span class="keyword">import</span> read_image</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomImageDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, annotations_file, img_dir, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">        <span class="variable language_">self</span>.img_dir = img_dir</span><br><span class="line">        <span class="variable language_">self</span>.transform = transform</span><br><span class="line">        <span class="variable language_">self</span>.target_transform = target_transform</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.img_labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_path = os.path.join(<span class="variable language_">self</span>.img_dir, <span class="variable language_">self</span>.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = read_image(img_path)</span><br><span class="line">        label = <span class="variable language_">self</span>.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.transform:</span><br><span class="line">            image = <span class="variable language_">self</span>.transform(image)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.target_transform:</span><br><span class="line">            label = target_transform(label)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure>
<p>接下来我们将详细解释每个函数的作用。</p>
<h4 id="init"><code>__init__</code></h4>
<p>当实例化 <code>Dataset</code> 对象时，<code>__init__</code> 函数只运行一次。我们在这里初始化图像所在的目录、标注文件的路径以及可能的变换操作（变换将在下一节详细介绍）。</p>
<p><code>labels.csv</code> 文件格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tshirt1.jpg, 0</span><br><span class="line">tshirt2.jpg, 0</span><br><span class="line">...</span><br><span class="line">ankleboot999.jpg, 9</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, annotations_file, img_dir, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># 读取标注文件（CSV格式），将其存储为一个 Pandas DataFrame</span></span><br><span class="line">    <span class="variable language_">self</span>.img_labels = pd.read_csv(annotations_file) </span><br><span class="line">    <span class="comment"># 存储图像文件所在的目录路径，后续通过文件名拼接完整路径来加载图像。</span></span><br><span class="line">    <span class="variable language_">self</span>.img_dir = img_dir </span><br><span class="line">    <span class="comment"># 可选参数：transform 是应用于图像的变换操作（如 ToTensor、Normalize 等）</span></span><br><span class="line">    <span class="variable language_">self</span>.transform = transform </span><br><span class="line">    <span class="comment"># 可选参数：target_transform 是应用于标签的变换操作</span></span><br><span class="line">    <span class="variable language_">self</span>.target_transform = target_transform </span><br></pre></td></tr></table></figure>
<p>如果你有自定义需求，比如图像增强，也可以传入自定义的 transform 函数。</p>
<h4 id="len"><code>__len__</code></h4>
<p><code>__len__</code> 函数返回数据集中样本的数量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.img_labels)</span><br></pre></td></tr></table></figure>
<h4 id="getitem"><code>__getitem__</code></h4>
<p><code>__getitem__</code> 函数根据给定索引 <code>idx</code> 从数据集中加载并返回一个样本。它根据索引确定图像在磁盘上的位置，然后使用 <code>read_image</code> 将其转换为张量，并从 <code>self.img_labels</code> 中获取对应的标签，然后调用相应的变换函数（如果有的话），最后返回图像和标签组成的元组。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">    <span class="comment"># 根据索引 idx 获取对应的图像文件名，并拼接成完整的图像路径</span></span><br><span class="line">    img_path = os.path.join(<span class="variable language_">self</span>.img_dir, <span class="variable language_">self</span>.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用 torchvision.io.read_image 读取图像文件，将其转换为张量（Tensor）</span></span><br><span class="line">    <span class="comment">#    read_image 返回的是 [C, H, W] 格式的张量（通道数、高度、宽度）</span></span><br><span class="line">    image = read_image(img_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取当前索引下的标签（即 CSV 文件中的第二列）</span></span><br><span class="line">    label = <span class="variable language_">self</span>.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果定义了图像变换（transform），就对图像进行变换操作，比如 Normalize(), RandomCrop() 等</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.transform:</span><br><span class="line">        image = <span class="variable language_">self</span>.transform(image)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果定义了标签变换（target_transform），就对标签进行变换操作，比如：将整数标签转换为 one-hot 编码等</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.target_transform:</span><br><span class="line">        label = <span class="variable language_">self</span>.target_transform(label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回处理后的图像张量和对应的标签元组</span></span><br><span class="line">    <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure>
<h3 id="使用-DataLoader-准备训练数据">使用 DataLoader 准备训练数据</h3>
<p><code>Dataset</code> 每次只返回一个样本的数据和标签。而在训练模型时，我们通常希望以“小批量”（minibatch）的方式传递样本，在每次训练周期（epoch）开始时打乱数据以减少过拟合风险，并利用 Python 的多进程加速数据加载。</p>
<p><code>DataLoader</code> 是一个封装了这些复杂逻辑的可迭代对象，提供了一个简单的 API。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>上面的代码将 <code>Dataset</code> 封装成可迭代的 <code>DataLoader</code>，用于批量加载数据。</p>
<ul>
<li><code>batch_size=64</code>：每次迭代返回 64 个样本（图像 + 标签）。</li>
<li><code>shuffle=True</code>：在每个 epoch 开始时打乱数据顺序，防止模型记住数据顺序，从而减少过拟合。</li>
</ul>
<h3 id="遍历-DataLoader">遍历 DataLoader</h3>
<p>我们已经将数据集加载到 <code>DataLoader</code> 中，可以根据需要进行遍历。每一次迭代都会返回一个批次的 <code>train_features</code> 和 <code>train_labels</code>（每个批次包含 <code>batch_size=64</code> 个样本）。由于设置了 <code>shuffle=True</code>，在遍历完所有批次之后，数据会被重新打乱（如需对数据加载顺序进行更细粒度的控制，请查看 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html#module-torch.utils.data.sampler">Samplers</a>）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train_features, train_labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader)) <span class="comment"># 从 DataLoader 中取出一个批次的数据</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Feature batch shape: <span class="subst">&#123;train_features.size()&#125;</span>&quot;</span>)  <span class="comment"># train_features 是一个张量，包含一批图像</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Labels batch shape: <span class="subst">&#123;train_labels.size()&#125;</span>&quot;</span>) <span class="comment"># train_labels 是对应的标签</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示第一张图像和它的标签</span></span><br><span class="line">img = train_features[<span class="number">0</span>].squeeze() <span class="comment"># squeeze() 移除张量中所有维度为 1 的轴，即将（1,28,28）变为 (28,28) 方便绘图</span></span><br><span class="line">label = train_labels[<span class="number">0</span>]</span><br><span class="line">plt.imshow(img, cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Label: <span class="subst">&#123;label&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://blog-img-save.oss-cn-chengdu.aliyuncs.com/img/file-20250524200823275.png" alt="|500"></p>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Feature batch shape: torch.Size([<span class="number">64</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>]) <span class="comment"># 表示这个批次中有 64 张图片，每张图片是 1 个通道，大小是 28*28 像素</span></span><br><span class="line">Labels batch shape: torch.Size([<span class="number">64</span>]) <span class="comment"># 64 个对应的标签</span></span><br><span class="line">Label: <span class="number">8</span></span><br></pre></td></tr></table></figure>
<h3 id="进一步阅读">进一步阅读</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html">torch.utils.data API 文档</a></li>
</ul>
<h2 id="数据变换">数据变换</h2>
<p>数据并不总是以适合训练机器学习模型的最终形式出现。我们使用 <strong>变换（transforms）</strong> 来对数据进行一些处理，使其更适合用于训练。</p>
<p>所有 TorchVision 数据集都提供两个参数：</p>
<ul>
<li><code>transform</code>：用于修改输入特征（图像）；</li>
<li><code>target_transform</code>：用于修改标签；</li>
</ul>
<p>这两个参数接受包含变换逻辑的可调用对象（如函数、lambda 表达式等）。<code>torchvision.transforms</code> 模块已经内置了多个常用的变换方法。</p>
<p>在本例中，FashionMNIST 数据集的图像是 PIL Image 格式，标签是整数。为了训练模型，我们需要将图像转换为归一化的张量，同时将标签转换为 one-hot 编码格式的张量。为此，我们使用 <code>ToTensor</code> 和 <code>Lambda</code> 变换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, Lambda</span><br><span class="line"></span><br><span class="line">ds = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor(),  <span class="comment"># 将图像转换为张量并归一化</span></span><br><span class="line">    target_transform=Lambda(<span class="keyword">lambda</span> y: torch.zeros(<span class="number">10</span>, dtype=torch.<span class="built_in">float</span>).scatter_(<span class="number">0</span>, torch.tensor(y), value=<span class="number">1</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="ToTensor"><code>ToTensor()</code></h3>
<p><code>ToTensor()</code> 将 PIL 图像或 NumPy 数组转换为 <code>FloatTensor</code>，并将像素值缩放到 <code>[0., 1.]</code> 范围内。</p>
<p>例如：</p>
<ul>
<li>原始图像像素范围是 <code>[0, 255]</code>；</li>
<li>经过 <code>ToTensor()</code> 后变为 <code>[0.0, 1.0]</code> 的浮点型张量；</li>
<li>张量形状从 <code>(H, W)</code> 变成 <code>(C, H, W)</code>，即通道优先格式（如 <code>[1, 28, 28]</code>）；</li>
</ul>
<h3 id="Lambda-变换">Lambda 变换</h3>
<p><code>Lambda</code> 允许你传入任意自定义的 lambda 函数来进行变换。</p>
<p>在这个例子中，我们定义了一个函数，将整数标签转换为 one-hot 编码的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">target_transform = Lambda(<span class="keyword">lambda</span> y: torch.zeros(</span><br><span class="line">    <span class="number">10</span>, dtype=torch.<span class="built_in">float</span>).scatter_(dim=<span class="number">0</span>, index=torch.tensor(y), value=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h4 id="功能说明：">功能说明：</h4>
<ol>
<li>创建一个大小为 10 的零张量（因为 FashionMNIST 有 10 个类别）；</li>
<li>使用 <code>scatter_()</code> 方法，在对应标签的位置上填入 1；</li>
<li>最终得到的是一个 one-hot 编码向量。</li>
</ol>
<p>示例：</p>
<p>如果原始标签 <code>y = 3</code>，那么经过该变换后变成：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]</span><br></pre></td></tr></table></figure>
<p>这表示第 4 类（索引从 0 开始）的 one-hot 编码。</p>
<h3 id="进一步阅读-2">进一步阅读</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/transforms.html">torchvision.transforms API 文档</a></li>
</ul>
<h2 id="构建神经网络">构建神经网络</h2>
<p>神经网络由执行数据操作的层/模块组成。</p>
<p><code>torch.nn</code> 命名空间提供了构建你自己的神经网络所需的所有基本组件。</p>
<p>每个 PyTorch 模块都继承自 <code>nn.Module</code>。一个神经网络本身就是一个模块，它包含其他模块（即层）。这种嵌套结构使得我们可以轻松地构建和管理复杂的架构。</p>
<p>在接下来的章节中，我们将构建一个用于对 FashionMNIST 数据集中的图像进行分类的神经网络。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br></pre></td></tr></table></figure>
<h3 id="获取训练设备">获取训练设备</h3>
<p>我们希望能够在诸如 CUDA、MPS、MTIA 或 XPU 这样的加速器上训练模型。如果当前加速器可用，我们将使用它；否则，我们使用 CPU。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.accelerator.current_accelerator().<span class="built_in">type</span> <span class="keyword">if</span> torch.accelerator.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;使用 <span class="subst">&#123;device&#125;</span> 设备&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用 cuda 设备</span><br></pre></td></tr></table></figure>
<h3 id="定义类">定义类</h3>
<p>我们通过继承 <code>nn.Module</code> 来定义我们的神经网络，并在 <code>__init__</code> 中初始化神经网络的各层。</p>
<p>每个 <code>nn.Module</code> 子类都会在 <code>forward</code> 方法中实现对输入数据的操作。当你写 <code>model(X)</code> 时，就是在调用这个函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个简单的前馈神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 展平层，将输入图像从形状 (batch_size, 28, 28) 变成 (batch_size, 28*28)。</span></span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten() </span><br><span class="line">        <span class="comment"># 网络的核心部分，由多个线性变换（全连接层）和激活函数 ReLU 组成</span></span><br><span class="line">        <span class="variable language_">self</span>.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>), <span class="comment"># 输入大小 784（28x28），输出大小 512 的全连接层</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>), <span class="comment"># 第二个全连接层，保持维度不变</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>), <span class="comment"># 输出层，输出 10 个类别的原始得分（logits）</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 前向传播函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x) <span class="comment"># 展平图像</span></span><br><span class="line">        logits = <span class="variable language_">self</span>.linear_relu_stack(x) <span class="comment"># 通过线性+ReLU堆栈</span></span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>
<p>我们创建一个 <code>NeuralNetwork</code> 的实例，并将其移动到指定的设备上，然后打印其结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (0): Linear(in_features=784, out_features=512, bias=True)</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>要使用该模型，我们只需将输入数据传入其中。这会自动执行模型的 <code>forward</code> 方法以及一些后台操作。<strong>不要直接调用 <code>model.forward()</code>！</strong></p>
<p>将输入数据传入模型后，模型返回的是一个 <strong>二维张量</strong>，其中 ：</p>
<ul>
<li><strong>dim=0</strong> 表示每个样本的输出；通常表示“批量”（batch size），即有多少个样本。</li>
<li><strong>dim=1</strong> 表示这个样本对每个类别的原始预测值（logits）。</li>
</ul>
<p>我们可以通过将结果传递给 <code>nn.Softmax</code> 模块来获得预测概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个形状为 `(1, 28, 28)` 的随机张量 `X`，表示一个模拟的输入图像</span></span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device=device) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 将这张图像 `X` 输入到神经网络模型中，输出一个二维向量，它们是原始的、未经过归一化的预测值</span></span><br><span class="line">logits = model(X) </span><br><span class="line"></span><br><span class="line"> <span class="comment"># 转换成概率分布</span></span><br><span class="line">pred_probab = nn.Softmax(dim=<span class="number">1</span>)(logits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找出概率最大的那个类别的索引</span></span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>) </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;预测类别: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">预测类别: tensor([9], device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure>
<h3 id="模型的各层详解">模型的各层详解</h3>
<p>让我们分解一下用于 FashionMNIST 分类的模型中的各层。为了说明这一点，我们将使用一个包含 3 张 28x28 图像的小批量样本，并观察当它通过网络时发生了什么。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_image = torch.rand(<span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(input_image.size())</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 28, 28])</span><br></pre></td></tr></table></figure>
<h4 id="nn-Flatten"><code>nn.Flatten</code></h4>
<p>我们初始化 <code>nn.Flatten</code> 层，将每个 2D 的 28x28 图像转换为一个连续的 784 像素值数组（保持 minibatch 维度不变）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flatten = nn.Flatten()</span><br><span class="line">flat_image = flatten(input_image)</span><br><span class="line"><span class="built_in">print</span>(flat_image.size())</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 784])</span><br></pre></td></tr></table></figure>
<h4 id="nn-Linear"><code>nn.Linear</code></h4>
<p>线性层是一个模块，它使用其存储的权重和偏置对输入进行线性变换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">layer1 = nn.Linear(in_features=<span class="number">28</span>*<span class="number">28</span>, out_features=<span class="number">20</span>)</span><br><span class="line">hidden1 = layer1(flat_image)</span><br><span class="line"><span class="built_in">print</span>(hidden1.size())</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 20])</span><br></pre></td></tr></table></figure>
<h4 id="nn-ReLU"><code>nn.ReLU</code></h4>
<p>非线性激活函数在输入与输出之间建立复杂映射关系。它们通常在线性变换之后使用，以引入非线性特性，帮助神经网络学习各种现象。</p>
<p>在这个模型中，我们在各个线性层之间使用了 <code>nn.ReLU</code>，但还有许多其他的激活函数可以用来引入非线性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;ReLU之前: <span class="subst">&#123;hidden1&#125;</span>\n\n&quot;</span>)</span><br><span class="line">hidden1 = nn.ReLU()(hidden1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;ReLU之后: <span class="subst">&#123;hidden1&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ReLU之前: tensor([[ 0.0104,  0.1062, -0.2085, ..., 0.6041],</span><br><span class="line">                 [ 0.1613,  0.2398, -0.1751, ..., 0.2209],</span><br><span class="line">                 [-0.0759,  0.1459, -0.3384, ..., 0.1549]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line"></span><br><span class="line">ReLU之后: tensor([[0.0104, 0.1062, 0.0000, ..., 0.6041],</span><br><span class="line">                [0.1613, 0.2398, 0.0000, ..., 0.2209],</span><br><span class="line">                [0.0000, 0.1459, 0.0000, ..., 0.1549]], grad_fn=&lt;ReluBackward0&gt;)</span><br></pre></td></tr></table></figure>
<h4 id="nn-Sequential"><code>nn.Sequential</code></h4>
<p><code>nn.Sequential</code> 是一个有序容器，用于将多个神经网络层按顺序组合在一起，形成一个完整的网络结构。数据按照定义顺序依次通过所有模块。你可以使用这个容器快速搭建网络结构。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">seq_modules = nn.Sequential(</span><br><span class="line">    flatten,</span><br><span class="line">    layer1,</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line">input_image = torch.rand(<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">logits = seq_modules(input_image)</span><br></pre></td></tr></table></figure>
<h4 id="nn-Softmax"><code>nn.Softmax</code></h4>
<p>神经网络的最后一层线性层输出的是 logits —— 范围从负无穷到正无穷的原始值。这些 logits 会被传入 <code>nn.Softmax</code> 模块。</p>
<p>Softmax 将 logits 缩放为 <code>[0, 1]</code> 区间的值，表示模型对每个类别的预测概率。<code>dim</code> 参数表示沿哪个维度进行归一化，使总和为 1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">pred_probab = softmax(logits)</span><br></pre></td></tr></table></figure>
<h3 id="模型参数">模型参数</h3>
<p>神经网络中的许多层是参数化的，即它们有相关的权重和偏置，在训练过程中会被优化。继承 <code>nn.Module</code> 后，PyTorch 会自动跟踪模型对象中定义的所有字段，并可通过 <code>parameters()</code> 或 <code>named_parameters()</code> 方法访问这些参数。</p>
<p>下面的例子中，我们遍历每个参数，并打印其大小及部分值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;模型结构: <span class="subst">&#123;model&#125;</span>\n\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;层: <span class="subst">&#123;name&#125;</span> | 大小: <span class="subst">&#123;param.size()&#125;</span> | 值 : <span class="subst">&#123;param[:<span class="number">2</span>]&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">模型结构: NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (0): Linear(in_features=784, out_features=512, bias=True)</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">层: linear_relu_stack.0.weight | 大小: torch.Size([512, 784]) | 值 : tensor([[ 0.0112, -0.0107,  0.0107,  ...,  0.0025,  0.0020, -0.0080],</span><br><span class="line">        [-0.0212, -0.0289, -0.0342,  ...,  0.0296,  0.0090,  0.0253]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">层: linear_relu_stack.0.bias | 大小: torch.Size([512]) | 值 : tensor([-0.0064, -0.0080], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="进一步阅读-3">进一步阅读</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html">torch.nn API 文档</a></li>
</ul>
<h2 id="使用-torch-autograd-进行自动微分">使用 <code>torch.autograd</code> 进行自动微分</h2>
<p>在训练神经网络时，最常用的算法是 <strong>反向传播（back propagation）</strong>。在这个算法中，参数（即模型的权重）会根据损失函数对相应参数的梯度进行调整。</p>
<p>为了计算这些梯度，PyTorch 提供了一个内置的自动微分引擎，叫做 <code>torch.autograd</code>。它支持对任意计算图进行梯度的自动计算。</p>
<p>考虑一个最简单的单层神经网络，输入为 <code>x</code>，参数为 <code>w</code> 和 <code>b</code>，并使用某个损失函数。这个结构可以用 PyTorch 定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">5</span>)  <span class="comment"># 输入张量</span></span><br><span class="line">y = torch.zeros(<span class="number">3</span>)  <span class="comment"># 期望输出</span></span><br><span class="line">w = torch.randn(<span class="number">5</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># 创建一个形状为 `(5, 3)` 的权重矩阵 `w`，其中的值是从标准正态分布中随机生成的。</span></span><br><span class="line">b = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># 创建一个形状为 `(3,)` 的偏置向量 `b`，值也是从正态分布中随机生成的。</span></span><br><span class="line">z = torch.matmul(x, w) + b <span class="comment"># 执行线性变换：`x` 和 `w` 做矩阵乘法（即 `x @ w`），然后加上偏置 `b`。`z` 是神经网络的原始输出（logits）。</span></span><br><span class="line">loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) <span class="comment"># 计算损失值</span></span><br></pre></td></tr></table></figure>
<h3 id="张量、函数与计算图">张量、函数与计算图</h3>
<p>这段代码定义了如下的计算图：</p>
<p><img src="https://blog-img-save.oss-cn-chengdu.aliyuncs.com/img/file-20250524214225458.png" alt="|500"></p>
<p>在这个网络中，<code>w</code> 和 <code>b</code> 是我们需要优化的参数。因此，我们需要能够计算损失函数关于这些变量的梯度。为了实现这一点，我们设置了这些张量的 <code>requires_grad</code> 属性。</p>
<blockquote>
<p>[!NOTE] 注意<br>
你可以通过在创建张量时设置 <code>requires_grad</code> 的值，或者之后使用 <code>x.requires_grad_(True)</code> 方法来启用梯度追踪。</p>
</blockquote>
<p>在 PyTorch 中，当你对张量进行操作（比如加法、乘法、矩阵乘等），这些操作会被记录下来，形成一个 <strong>计算图（computational graph）</strong> ，用于后续自动求导（autograd）。这个图是 PyTorch 在后台自动构建的。</p>
<p>每个操作都会被封装成一个 <code>Function</code> 类的对象，这个对象知道如何在<strong>前向传播</strong>（forward）过程中计算该函数，也知道如何在<strong>反向传播</strong>（backward）步骤中计算其导数。</p>
<p>这个反向传播函数的引用就保存在张量的 <code>.grad_fn</code> 属性中。你可以在这篇文档中找到关于 <code>Function</code> 的更多信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;z 的梯度函数: <span class="subst">&#123;z.grad_fn&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;loss 的梯度函数: <span class="subst">&#123;loss.grad_fn&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z 的梯度函数: &lt;AddBackward0 object at 0x7f85df28cb50&gt;</span><br><span class="line">loss 的梯度函数: &lt;BinaryCrossEntropyWithLogitsBackward0 object at 0x7f85df25af20&gt;</span><br></pre></td></tr></table></figure>
<h3 id="计算梯度">计算梯度</h3>
<p>为了优化神经网络中的参数权重，我们需要计算损失函数相对于参数的导数，也就是我们要计算：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac><mspace width="1em"/><mtext>和</mtext><mspace width="1em"/><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>b</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial loss}{\partial w} \quad \text{和} \quad \frac{\partial loss}{\partial b}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.0574em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">oss</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:1em;"></span><span class="mord text"><span class="mord cjk_fallback">和</span></span><span class="mspace" style="margin-right:1em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">b</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">oss</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>在给定的 <code>x</code> 和 <code>y</code> 值下。为了计算这些导数，我们调用 <code>loss.backward()</code>，然后从 <code>w.grad</code> 和 <code>b.grad</code> 中获取结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss.backward() <span class="comment"># 计算损失函数对模型参数的梯度</span></span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(b.grad)</span><br></pre></td></tr></table></figure>
<p>当你调用 <code>loss.backward()</code> 时，PyTorch 会从 <code>loss</code> 开始，沿着计算图一路回溯，使用每个节点的 <code>.grad_fn</code> 来自动计算出所有带有 <code>requires_grad=True</code> 的张量（如 <code>w</code> 和 <code>b</code>）的梯度，并将它们存储在 <code>.grad</code> 属性中。这就是 PyTorch 的 <strong>自动微分机制（autograd）</strong> 的核心。</p>
<blockquote>
<p>[!NOTE]</p>
<p>我们只能获取计算图中“叶节点”（leaf nodes）的 <code>grad</code> 属性，这些节点的 <code>requires_grad</code> 设置为 <code>True</code>。对于图中的其他节点，梯度不可用。</p>
<p>出于性能原因，默认情况下，我们只能对给定的计算图调用一次 <code>backward()</code>。如果我们需要在同一张图上多次调用 <code>backward()</code>，我们需要传入 <code>retain_graph=True</code> 参数。</p>
</blockquote>
<p>输出示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.3275, 0.2701, 0.1925],</span><br><span class="line">        [0.3275, 0.2701, 0.1925],</span><br><span class="line">        [0.3275, 0.2701, 0.1925],</span><br><span class="line">        [0.3275, 0.2701, 0.1925],</span><br><span class="line">        [0.3275, 0.2701, 0.1925]])</span><br><span class="line">tensor([0.3275, 0.2701, 0.1925])</span><br></pre></td></tr></table></figure>
<h3 id="禁用梯度追踪">禁用梯度追踪</h3>
<p>默认情况下，所有 <code>requires_grad=True</code> 的张量都会记录它们的计算历史并支持梯度计算。但在某些情况下我们不需要这样做，例如当我们已经训练好模型并只想进行推理（即只做前向计算）时。我们可以使用 <code>torch.no_grad()</code> 上下文管理器来禁用梯度追踪：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z = torch.matmul(x, w) + b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    z = torch.matmul(x, w) + b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure>
<p>另一种实现相同效果的方法是对张量使用 <code>detach()</code> 方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z = torch.matmul(x, w) + b</span><br><span class="line">z_det = z.detach()</span><br><span class="line"><span class="built_in">print</span>(z_det.requires_grad)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure>
<p>你可能会想要禁用梯度追踪的原因包括：</p>
<ul>
<li>将神经网络中的某些参数标记为<strong>冻结参数</strong>（frozen parameters）。</li>
<li>在仅执行前向传播时加快计算速度，因为不追踪梯度的张量运算效率更高。</li>
</ul>
<h3 id="更多关于计算图的内容">更多关于计算图的内容</h3>
<p>从概念上讲，<code>autograd</code> 会记录数据（张量）和所有执行的操作（以及由此生成的新张量），构成一个有向无环图（DAG），其中包含多个 <code>Function</code> 对象。在这个 DAG 中，叶子是输入张量，根是输出张量。通过从根到叶子遍历这张图，可以使用链式法则自动计算梯度。</p>
<p>在前向传播过程中，<code>autograd</code> 同时完成两件事：</p>
<ol>
<li>执行请求的操作以计算结果张量；</li>
<li>在 DAG 中维护该操作的梯度函数。</li>
</ol>
<p>当在 DAG 的根节点上调用 <code>.backward()</code> 时，开始反向传播过程。<code>autograd</code> 会：</p>
<ul>
<li>计算每个 <code>.grad_fn</code> 的梯度；</li>
<li>将它们累加到对应张量的 <code>.grad</code> 属性中；</li>
<li>使用链式法则，一直传播到叶子张量。</li>
</ul>
<blockquote>
<p>[!NOTE]<br>
PyTorch 中的 DAG 是动态的。一个重要特点是每次 <code>.backward()</code> 调用后，图都会重新构建。这使得你可以在模型中使用控制流语句；如果你需要，可以在每次迭代中更改形状、大小和操作。</p>
</blockquote>
<h3 id="可选阅读：张量梯度与雅可比矩阵乘积">可选阅读：张量梯度与雅可比矩阵乘积</h3>
<p>在很多情况下，我们有一个标量损失函数，并需要计算相对于某些参数的梯度。但在有些情况下，输出函数是一个任意的张量。在这种情况下，PyTorch 允许你计算所谓的 <strong>雅可比矩阵乘积（Jacobian product）</strong>，而不是实际的梯度。</p>
<p>对于向量函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>⃗</mo></mover><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mover accent="true"><mi>x</mi><mo>⃗</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\vec{y} = f(\vec{x})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9084em;vertical-align:-0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.714em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1799em;"><span class="overlay" style="height:0.714em;width:0.471em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.471em" height="0.714em" style="width:0.471em" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"/></svg></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.714em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2077em;"><span class="overlay" style="height:0.714em;width:0.471em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.471em" height="0.714em" style="width:0.471em" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"/></svg></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>⃗</mo></mover><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">⟩</mo></mrow><annotation encoding="application/x-tex">\vec{x} = \langle x_1, ..., x_n \rangle</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.714em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.714em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2077em;"><span class="overlay" style="height:0.714em;width:0.471em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.471em" height="0.714em" style="width:0.471em" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"/></svg></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">⟨</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">⟩</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>⃗</mo></mover><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>y</mi><mi>m</mi></msub><mo stretchy="false">⟩</mo></mrow><annotation encoding="application/x-tex">\vec{y} = \langle y_1, ..., y_m \rangle</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9084em;vertical-align:-0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.714em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1799em;"><span class="overlay" style="height:0.714em;width:0.471em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.471em" height="0.714em" style="width:0.471em" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"/></svg></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">⟨</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">⟩</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9084em;vertical-align:-0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.714em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1799em;"><span class="overlay" style="height:0.714em;width:0.471em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.471em" height="0.714em" style="width:0.471em" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"/></svg></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span></span></span></span> 相对于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.714em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.714em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2077em;"><span class="overlay" style="height:0.714em;width:0.471em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.471em" height="0.714em" style="width:0.471em" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"/></svg></span></span></span></span></span></span></span></span></span></span> 的梯度由雅可比矩阵给出：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>J</mi><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>y</mi><mn>1</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋯</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>y</mi><mn>1</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>x</mi><mi>n</mi></msub></mrow></mfrac></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">⋮</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋱</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">⋮</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>y</mi><mi>m</mi></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋯</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>y</mi><mi>m</mi></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>x</mi><mi>n</mi></msub></mrow></mfrac></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">J = 
\begin{pmatrix}
\frac{\partial y_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial y_1}{\partial x_n} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial y_m}{\partial x_1} &amp; \cdots &amp; \frac{\partial y_m}{\partial x_n}
\end{pmatrix}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:4.6146em;vertical-align:-2.0573em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.35em;"><span style="top:-4.35em;"><span class="pstrut" style="height:6.2em;"></span><span style="width:0.875em;height:4.200em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.875em" height="4.200em" viewBox="0 0 875 4200"><path d="M863,9c0,-2,-2,-5,-6,-9c0,0,-17,0,-17,0c-12.7,0,-19.3,0.3,-20,1
c-5.3,5.3,-10.3,11,-15,17c-242.7,294.7,-395.3,682,-458,1162c-21.3,163.3,-33.3,349,
-36,557 l0,684c0.2,6,0,26,0,60c2,159.3,10,310.7,24,454c53.3,528,210,
949.7,470,1265c4.7,6,9.7,11.7,15,17c0.7,0.7,7,1,19,1c0,0,18,0,18,0c4,-4,6,-7,6,-9
c0,-2.7,-3.3,-8.7,-10,-18c-135.3,-192.7,-235.5,-414.3,-300.5,-665c-65,-250.7,-102.5,
-544.7,-112.5,-882c-2,-104,-3,-167,-3,-189
l0,-692c0,-162.7,5.7,-314,17,-454c20.7,-272,63.7,-513,129,-723c65.3,
-210,155.3,-396.3,270,-559c6.7,-9.3,10,-15.3,10,-18z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.85em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5573em;"><span style="top:-5.3126em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4451em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.3675em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0em;border-top-width:1.5em;bottom:0em;"></span></span></span></span><span style="top:-2.0753em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4451em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.0573em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5573em;"><span style="top:-5.1251em;"><span class="pstrut" style="height:3.5em;"></span><span class="mord"><span class="minner">⋯</span></span></span><span style="top:-3.18em;"><span class="pstrut" style="height:3.5em;"></span><span class="mord"><span class="minner">⋱</span></span></span><span style="top:-1.8878em;"><span class="pstrut" style="height:3.5em;"></span><span class="mord"><span class="minner">⋯</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.0573em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5573em;"><span style="top:-5.3126em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4451em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.3675em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0em;border-top-width:1.5em;bottom:0em;"></span></span></span></span><span style="top:-2.0753em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4451em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.0573em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.35em;"><span style="top:-4.35em;"><span class="pstrut" style="height:6.2em;"></span><span style="width:0.875em;height:4.200em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.875em" height="4.200em" viewBox="0 0 875 4200"><path d="M76,0c-16.7,0,-25,3,-25,9c0,2,2,6.3,6,13c21.3,28.7,42.3,60.3,
63,95c96.7,156.7,172.8,332.5,228.5,527.5c55.7,195,92.8,416.5,111.5,664.5
c11.3,139.3,17,290.7,17,454c0,28,1.7,43,3.3,45l0,609
c-3,4,-3.3,16.7,-3.3,38c0,162,-5.7,313.7,-17,455c-18.7,248,-55.8,469.3,-111.5,664
c-55.7,194.7,-131.8,370.3,-228.5,527c-20.7,34.7,-41.7,66.3,-63,95c-2,3.3,-4,7,-6,11
c0,7.3,5.7,11,17,11c0,0,11,0,11,0c9.3,0,14.3,-0.3,15,-1c5.3,-5.3,10.3,-11,15,-17
c242.7,-294.7,395.3,-681.7,458,-1161c21.3,-164.7,33.3,-350.7,36,-558
l0,-744c-2,-159.3,-10,-310.7,-24,-454c-53.3,-528,-210,-949.7,
-470,-1265c-4.7,-6,-9.7,-11.7,-15,-17c-0.7,-0.7,-6.7,-1,-18,-1z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.85em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>PyTorch 不直接计算雅可比矩阵本身，而是允许你计算雅可比乘积 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>v</mi><mi>T</mi></msup><mo>⋅</mo><mi>J</mi></mrow><annotation encoding="application/x-tex">v^T \cdot J</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>v</mi><mn>1</mn></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>v</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">v = (v_1 ... v_m)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">...</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 是一个给定的输入向量。这可以通过将 <code>v</code> 作为参数传递给 <code>backward()</code> 来实现。<code>v</code> 的大小应与原始张量相同，以便计算乘积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inp = torch.eye(<span class="number">4</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">out = (inp + <span class="number">1</span>).<span class="built_in">pow</span>(<span class="number">2</span>).t()</span><br><span class="line">out.backward(torch.ones_like(out), retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;第一次调用\n<span class="subst">&#123;inp.grad&#125;</span>&quot;</span>)</span><br><span class="line">out.backward(torch.ones_like(out), retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\n第二次调用\n<span class="subst">&#123;inp.grad&#125;</span>&quot;</span>)</span><br><span class="line">inp.grad.zero_()</span><br><span class="line">out.backward(torch.ones_like(out), retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\n清零梯度后的调用\n<span class="subst">&#123;inp.grad&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">第一次调用</span><br><span class="line">tensor([[4., 2., 2., 2., 2.],</span><br><span class="line">        [2., 4., 2., 2., 2.],</span><br><span class="line">        [2., 2., 4., 2., 2.],</span><br><span class="line">        [2., 2., 2., 4., 2.]])</span><br><span class="line"></span><br><span class="line">第二次调用</span><br><span class="line">tensor([[8., 4., 4., 4., 4.],</span><br><span class="line">        [4., 8., 4., 4., 4.],</span><br><span class="line">        [4., 4., 8., 4., 4.],</span><br><span class="line">        [4., 4., 4., 8., 4.]])</span><br><span class="line"></span><br><span class="line">清零梯度后的调用</span><br><span class="line">tensor([[4., 2., 2., 2., 2.],</span><br><span class="line">        [2., 4., 2., 2., 2.],</span><br><span class="line">        [2., 2., 4., 2., 2.],</span><br><span class="line">        [2., 2., 2., 4., 2.]])</span><br></pre></td></tr></table></figure>
<p>请注意，当你使用相同的参数第二次调用 <code>backward()</code> 时，梯度值会发生变化。这是因为反向传播过程中，PyTorch 会累积梯度，即计算出的梯度会被加到所有叶子节点的 <code>grad</code> 属性中。如果你想得到正确的梯度，你需要在每次计算前清空 <code>grad</code> 属性。在实际训练中，优化器会帮助我们完成这一操作。</p>
<blockquote>
<p>[!NOTE]<br>
之前我们调用 <code>backward()</code> 时不带参数。这实际上等价于调用 <code>backward(torch.tensor(1.0))</code>，这是在标量函数（如神经网络训练中的损失函数）中计算梯度的一种有效方式。</p>
</blockquote>
<h3 id="进一步阅读-4">进一步阅读</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/autograd.html">Autograd 机制</a></li>
</ul>
<h2 id="优化模型参数-2">优化模型参数</h2>
<p>现在我们已经构建了模型和数据集，接下来就是通过在数据上优化模型参数来<strong>训练、验证和测试我们的模型</strong>。训练模型是一个迭代的过程；在每次迭代中，模型会猜测输出结果，计算预测误差（loss），收集误差相对于模型参数的导数（如前一节所述），并使用梯度下降法优化这些参数。如果你想更详细地了解这个过程，可以观看 <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=tIeHLnjs5U8">3Blue1Brown 关于反向传播的视频</a>。</p>
<h3 id="前置代码">前置代码</h3>
<p>我们使用之前章节中的代码：<a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%99%A8">数据集与数据加载器</a> 和<a href="#%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">构建模型</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练数据和测试数据</span></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 DataLoaders</span></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()</span><br><span class="line">        <span class="variable language_">self</span>.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x)</span><br><span class="line">        logits = <span class="variable language_">self</span>.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">model = NeuralNetwork()</span><br></pre></td></tr></table></figure>
<h3 id="超参数">超参数</h3>
<p>超参数是可以调整的参数，它们控制着模型优化过程。不同的超参数值可能会影响模型训练效果和收敛速度（了解更多关于<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html">超参数调优</a>）。</p>
<p>我们为训练定义以下超参数：</p>
<ul>
<li><strong>Epoch 数量（Epochs）</strong>：对整个数据集进行迭代的次数。</li>
<li><strong>批量大小（Batch Size）</strong>：在更新模型参数之前，通过网络传播的数据样本数量。</li>
<li><strong>学习率（Learning Rate）</strong>：在每个批次 epoch 中，模型参数更新的幅度。较小的学习率会导致学习缓慢，而较大的学习率可能导致训练过程中行为不稳定。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">1e-3</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">epochs = <span class="number">5</span></span><br></pre></td></tr></table></figure>
<h3 id="优化循环">优化循环</h3>
<p>设置好超参数后，我们可以使用一个优化循环来训练和优化模型。优化循环的每一次迭代称为一个 epoch。</p>
<p>每个 epoch 主要包括两个部分：</p>
<ol>
<li><strong>训练循环（Train Loop）</strong>：遍历训练数据集，尝试收敛到最优参数。</li>
<li><strong>验证/测试循环（Validation/Test Loop）</strong>：遍历测试数据集，检查模型性能是否提升。</li>
</ol>
<p>让我们先简要熟悉一下训练循环中使用的一些概念。跳转下文查看<a href="#%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0">完整的优化循环实现</a>。</p>
<h4 id="损失函数（Loss-Function）">损失函数（Loss Function）</h4>
<p>当给定一些训练数据时，未训练的网络很可能不会给出正确答案。损失函数用于衡量预测结果与真实标签之间的差异，并且是我们希望在训练过程中最小化的对象。为了计算损失，我们使用输入数据做出预测，并将其与真实标签对比。</p>
<p>常用的损失函数包括：</p>
<ul>
<li><code>nn.MSELoss</code>（均方误差）——适用于回归任务；</li>
<li><code>nn.NLLLoss</code>（负对数似然）——适用于分类任务；</li>
<li><code>nn.CrossEntropyLoss</code> 是 <code>nn.LogSoftmax</code> 和 <code>nn.NLLLoss</code> 的组合。</li>
</ul>
<p>我们将模型的输出 logits 传入 <code>nn.CrossEntropyLoss</code>，它将对 logits 进行归一化并计算预测误差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<h4 id="优化器（Optimizer）">优化器（Optimizer）</h4>
<p>优化是指在每次训练步骤中调整模型参数以减少模型误差的过程。优化算法定义了这一过程的执行方式（在这个例子中我们使用随机梯度下降 SGD）。所有优化逻辑都封装在优化器对象中。在这里，我们使用 <code>SGD</code> 优化器；此外，PyTorch 还提供了许多<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">其他优化器</a>，例如 ADAM 和 RMSProp，适用于不同类型的模型和数据。</p>
<p>我们通过注册需要训练的模型参数，并传递学习率超参数来初始化优化器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br></pre></td></tr></table></figure>
<p>在训练循环中，优化分为三个步骤：</p>
<ol>
<li>调用 <code>optimizer.zero_grad()</code> 来重置模型参数的梯度。默认情况下梯度会累加，为防止重复计数，我们在每次迭代时显式地将它们清零。</li>
<li>反向传播预测损失：调用 <code>loss.backward()</code>。PyTorch 会将损失相对于每个参数的梯度存储下来。</li>
<li>得到梯度后，调用 <code>optimizer.step()</code> 来根据反向传播收集的梯度调整参数。</li>
</ol>
<h3 id="完整实现">完整实现</h3>
<p>我们定义 <code>train_loop</code> 函数来进行训练，以及 <code>test_loop</code> 函数来评估模型在测试数据上的性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_loop</span>(<span class="params">dataloader, model, loss_fn, optimizer</span>):</span><br><span class="line">    <span class="comment"># 设置模型为训练模式（train mode），这对包含 BatchNorm 和 Dropout 层的模型非常重要</span></span><br><span class="line">    <span class="comment"># 即使在这个例子中可能不是必须的，但为了最佳实践我们还是加上了</span></span><br><span class="line">    model.train()</span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)    <span class="comment"># 获取整个数据集的大小，用于打印进度</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> batch, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        <span class="comment"># 计算预测值和损失</span></span><br><span class="line">        pred = model(X)    <span class="comment"># 模型预测输出</span></span><br><span class="line">        loss = loss_fn(pred, y)    <span class="comment"># 计算当前 batch 的损失</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播：计算梯度并更新参数</span></span><br><span class="line">        loss.backward()        <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">        optimizer.step()        <span class="comment"># 更新模型参数 </span></span><br><span class="line">        optimizer.zero_grad()    <span class="comment"># 清空当前梯度</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每处理100个 batch 打印一次当前的训练进度</span></span><br><span class="line">        <span class="keyword">if</span> batch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            loss, current = loss.item(), batch * batch_size + <span class="built_in">len</span>(X)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;loss: <span class="subst">&#123;loss:&gt;7f&#125;</span>  [<span class="subst">&#123;current:&gt;5d&#125;</span>/<span class="subst">&#123;size:&gt;5d&#125;</span>]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_loop</span>(<span class="params">dataloader, model, loss_fn</span>):</span><br><span class="line">    <span class="comment"># 设置模型为评估模式（evaluation mode），同样是为了 BatchNorm 和 Dropout 层的正确行为</span></span><br><span class="line">    <span class="comment"># 虽然在此示例中可能不必要，但为了规范我们仍保留</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">    num_batches = <span class="built_in">len</span>(dataloader)</span><br><span class="line">    test_loss, correct = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在评估过程中禁用梯度计算，提高效率并节省内存</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> dataloader:</span><br><span class="line">            pred = model(X)    <span class="comment"># 模型预测输出</span></span><br><span class="line">            test_loss += loss_fn(pred, y).item()    <span class="comment"># 累加每个batch的损失</span></span><br><span class="line">            correct += (pred.argmax(<span class="number">1</span>) == y).<span class="built_in">type</span>(torch.<span class="built_in">float</span>).<span class="built_in">sum</span>().item() <span class="comment"># 计算预测正确的样本总数</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 计算平均损失和准确率</span></span><br><span class="line">    test_loss /= num_batches</span><br><span class="line">    correct /= size</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Test Error: \n Accuracy: <span class="subst">&#123;(<span class="number">100</span>*correct):&gt;<span class="number">0.1</span>f&#125;</span>%, Avg loss: <span class="subst">&#123;test_loss:&gt;8f&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>然后我们初始化损失函数和优化器，并传递给 <code>train_loop</code> 和 <code>test_loop</code>。你可以增加训练轮次来观察模型性能的逐步提升：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;t+<span class="number">1</span>&#125;</span>\n-------------------------------&quot;</span>)</span><br><span class="line">    train_loop(train_dataloader, model, loss_fn, optimizer)</span><br><span class="line">    test_loop(test_dataloader, model, loss_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Done!&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>以下是训练过程的部分输出示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1</span><br><span class="line">-------------------------------</span><br><span class="line">loss: 2.313167  [   64/60000]</span><br><span class="line">...</span><br><span class="line">Test Error:</span><br><span class="line"> Accuracy: 42.2%, Avg loss: 2.165621</span><br><span class="line"></span><br><span class="line">Epoch 2</span><br><span class="line">-------------------------------</span><br><span class="line">loss: 2.175947  [   64/60000]</span><br><span class="line">...</span><br><span class="line">Test Error:</span><br><span class="line"> Accuracy: 55.2%, Avg loss: 1.917092</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">Epoch 10</span><br><span class="line">-------------------------------</span><br><span class="line">loss: 0.819730  [   64/60000]</span><br><span class="line">...</span><br><span class="line">Test Error:</span><br><span class="line"> Accuracy: 71.6%, Avg loss: 0.783405</span><br><span class="line">Done!</span><br></pre></td></tr></table></figure>
<h3 id="进一步阅读-5">进一步阅读</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#loss-functions">损失函数</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">torch.optim 文档</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html">模型预热训练（Warmstart Training）</a></li>
</ul>
<h2 id="保存并加载模型">保存并加载模型</h2>
<p>在本节中，我们将学习如何<strong>持久化模型状态</strong>，包括保存、加载模型以及运行模型预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br></pre></td></tr></table></figure>
<h3 id="保存和加载模型权重">保存和加载模型权重</h3>
<p>PyTorch 模型将其学习到的参数存储在一个名为 <code>state_dict</code> 的内部状态字典中。我们可以使用 <code>torch.save</code> 方法将这些参数持久化保存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个预训练的 VGG16 神经网络模型，并加载在 ImageNet 数据集上训练好的权重</span></span><br><span class="line">model = models.vgg16(weights=<span class="string">&#x27;IMAGENET1K_V1&#x27;</span>) </span><br><span class="line"><span class="comment"># 保存模型权重</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model_weights.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>输出（下载预训练模型时）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Downloading: &quot;https://download.pytorch.org/models/vgg16-397923af.pth&quot; to /var/lib/ci-user/.cache/torch/hub/checkpoints/vgg16-397923af.pth</span><br><span class="line"></span><br><span class="line">  0%|          | 0.00/528M [00:00&lt;?, ?B/s]</span><br><span class="line"> ...</span><br><span class="line">100%|##########| 528M/528M [00:01&lt;00:00, 451MB/s]</span><br></pre></td></tr></table></figure>
<p>要加载模型权重，你首先需要创建一个与原模型结构相同的模型实例，然后使用 <code>load_state_dict()</code> 方法加载参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = models.vgg16()  <span class="comment"># 不指定 weights，即创建未训练的模型</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model_weights.pth&#x27;</span>, weights_only=<span class="literal">True</span>))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>提示：</strong><br>
在反序列化过程中，我们设置 <code>weights_only=True</code>，以限制只执行与加载权重相关的函数。当仅加载权重时，这是推荐的最佳实践。</p>
</blockquote>
<p>输出（模型结构）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    ...</span><br><span class="line">    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Linear(in_features=25088, out_features=4096, bias=True)</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): Dropout(p=0.5, inplace=False)</span><br><span class="line">    ...</span><br><span class="line">    (6): Linear(in_features=4096, out_features=1000, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>⚠️ <strong>注意：</strong></p>
<p>在进行推理之前，一定要调用 <code>model.eval()</code> 方法，将 dropout 和 batch normalization 层设为评估模式。如果不这样做，会导致推理结果不一致。</p>
</blockquote>
<h3 id="保存和加载带有结构的模型">保存和加载带有结构的模型</h3>
<p>在加载模型权重时，我们需要先实例化相同结构的模型类，因为该类定义了网络的结构。有时我们也希望将模型的结构与权重一起保存。在这种情况下，可以将整个模型对象（而不是 <code>state_dict</code>）传递给保存函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model, <span class="string">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>然后可以像下面这样加载模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.load(<span class="string">&#x27;model.pth&#x27;</span>, weights_only=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>📌 <strong>说明：</strong><br>
这种方法在序列化模型时会使用 Python 的 <code>pickle</code> 模块，因此在加载模型时必须能够访问原始类的定义。</p>
</blockquote>
<p>如 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/main/notes/serialization.html#saving-and-loading-torch-nn-modules">Saving and loading torch.nn.Modules </a> 所述，保存 <code>state_dict</code> 被认为是最佳实践。但在这种情况下，我们需要使用 <code>weights_only=False</code>，因为这是加载整个模型的一种遗留方式。</p>
<h3 id="相关教程">相关教程</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html">在 PyTorch 中保存和加载通用检查点</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/recipes/recipes/module_load_state_dict_tips.html?highlight=loading%20nn%20module%20from%20checkpoint">从检查点加载 nn.Module 的技巧</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://zyw9825.github.io">zyw9825</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://zyw9825.github.io/2025/05/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PyTorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">https://zyw9825.github.io/2025/05/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PyTorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://zyw9825.github.io" target="_blank">代码手记</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/PyTroch/">PyTroch</a></div><div class="post-share"><div class="social-share" data-image="https://blog-img-save.oss-cn-chengdu.aliyuncs.com/bg/BG/wukong.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/05/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/" title="小波变换"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">小波变换</div></div><div class="info-2"><div class="info-item-1">小波变换到底变换什么？ 在信号处理中，数学变换通常应用于信号，以获取原始信号中不容易获得的更多信息。 这里得原始信号是指时域信号。当我们绘制绝大多数信号时，横轴往往是时间，而纵轴往往是振幅。 这种时间-振幅的信号表示方法，并不总是最合适的，很多时候，最重要的信息隐藏在信号的频率内容中。比如：  生物医学信号 ：如心电图（ECG）、脑电图（EEG）等，通过频谱分析可以检测异常的生理状态。 电力信号 ：电网中的电流通常是 50Hz 或 60Hz 的正弦波，如果出现其他频率成分，可能意味着电路故障。  频率表示单位时间内某个周期性事件发生的次数，单位是周期/秒，也称为赫兹 Hz。在信号处理中，频率通常用来描述信号的变化速度。如果一个变量快速变化，我们说它是高频的，如果一个变量缓慢变化，即平滑地变化，我们说它是低频的。信号的频谱显示了该信号中存在哪些频率。 信号的频率成分要怎么测量呢？ 要测量信号的频率成分，最常用的方法是使用傅里叶变换（Fourier Transform, FT）...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="https://blog-img-save.oss-cn-chengdu.aliyuncs.com/bg/BG/wukong.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">zyw9825</div><div class="author-info-description">你且迷这风浪永远二十赶朝暮</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zyw9825"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/zyw9825" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:zyw_9825@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=765830653&amp;website=www.oicqzone.com" target="_blank" title="添加QQ好友"><i class="fa-brands fa-qq"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">PyTorch 简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0-PyTorch-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="toc-number">2.</span> <span class="toc-text">学习 PyTorch 基础知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8"><span class="toc-number">2.1.</span> <span class="toc-text">快速入门</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE"><span class="toc-number">2.1.1.</span> <span class="toc-text">处理数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.2.</span> <span class="toc-text">创建模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">2.1.3.</span> <span class="toc-text">优化模型参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.4.</span> <span class="toc-text">保存模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.5.</span> <span class="toc-text">加载模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F"><span class="toc-number">2.2.</span> <span class="toc-text">张量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%BC%A0%E9%87%8F"><span class="toc-number">2.2.1.</span> <span class="toc-text">初始化张量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B4%E6%8E%A5%E4%BB%8E%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%88%9B%E5%BB%BA"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">直接从数据中创建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8E-NumPy-%E6%95%B0%E7%BB%84%E5%88%9B%E5%BB%BA"><span class="toc-number">2.2.1.2.</span> <span class="toc-text">从 NumPy 数组创建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8E%E5%8F%A6%E4%B8%80%E4%B8%AA%E5%BC%A0%E9%87%8F%E5%88%9B%E5%BB%BA"><span class="toc-number">2.2.1.3.</span> <span class="toc-text">从另一个张量创建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E9%9A%8F%E6%9C%BA%E5%80%BC%E6%88%96%E5%B8%B8%E6%95%B0%E5%80%BC%E5%88%9B%E5%BB%BA"><span class="toc-number">2.2.1.4.</span> <span class="toc-text">使用随机值或常数值创建</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B1%9E%E6%80%A7"><span class="toc-number">2.2.2.</span> <span class="toc-text">张量的属性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E5%BC%A0%E9%87%8F%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.3.</span> <span class="toc-text">对张量的操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%87%E5%87%86%E7%9A%84-NumPy-%E9%A3%8E%E6%A0%BC%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87"><span class="toc-number">2.2.3.1.</span> <span class="toc-text">标准的 NumPy 风格索引和切片</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8B%BC%E6%8E%A5%E5%BC%A0%E9%87%8F"><span class="toc-number">2.2.3.2.</span> <span class="toc-text">拼接张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%9C%AF%E8%BF%90%E7%AE%97"><span class="toc-number">2.2.3.3.</span> <span class="toc-text">算术运算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95%E4%B8%AA%E5%85%83%E7%B4%A0%E5%BC%A0%E9%87%8F"><span class="toc-number">2.2.3.4.</span> <span class="toc-text">单个元素张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%9F%E5%9C%B0%E6%93%8D%E4%BD%9C%EF%BC%88In-place-Operations%EF%BC%89"><span class="toc-number">2.2.3.5.</span> <span class="toc-text">原地操作（In-place Operations）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E-NumPy-%E7%9A%84%E6%A1%A5%E6%8E%A5"><span class="toc-number">2.2.4.</span> <span class="toc-text">与 NumPy 的桥接</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E8%BD%AC-NumPy-%E6%95%B0%E7%BB%84"><span class="toc-number">2.2.4.1.</span> <span class="toc-text">张量转 NumPy 数组</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NumPy-%E6%95%B0%E7%BB%84%E8%BD%AC%E5%BC%A0%E9%87%8F"><span class="toc-number">2.2.4.2.</span> <span class="toc-text">NumPy 数组转张量</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%99%A8"><span class="toc-number">2.3.</span> <span class="toc-text">数据集与数据加载器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.3.1.</span> <span class="toc-text">加载数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%81%8D%E5%8E%86%E5%B9%B6%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.3.2.</span> <span class="toc-text">遍历并可视化数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.3.3.</span> <span class="toc-text">创建自定义数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#init"><span class="toc-number">2.3.3.1.</span> <span class="toc-text">__init__</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#len"><span class="toc-number">2.3.3.2.</span> <span class="toc-text">__len__</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#getitem"><span class="toc-number">2.3.3.3.</span> <span class="toc-text">__getitem__</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-DataLoader-%E5%87%86%E5%A4%87%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-number">2.3.4.</span> <span class="toc-text">使用 DataLoader 准备训练数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%81%8D%E5%8E%86-DataLoader"><span class="toc-number">2.3.5.</span> <span class="toc-text">遍历 DataLoader</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E9%98%85%E8%AF%BB"><span class="toc-number">2.3.6.</span> <span class="toc-text">进一步阅读</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8F%98%E6%8D%A2"><span class="toc-number">2.4.</span> <span class="toc-text">数据变换</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ToTensor"><span class="toc-number">2.4.1.</span> <span class="toc-text">ToTensor()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lambda-%E5%8F%98%E6%8D%A2"><span class="toc-number">2.4.2.</span> <span class="toc-text">Lambda 变换</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%9F%E8%83%BD%E8%AF%B4%E6%98%8E%EF%BC%9A"><span class="toc-number">2.4.2.1.</span> <span class="toc-text">功能说明：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E9%98%85%E8%AF%BB-2"><span class="toc-number">2.4.3.</span> <span class="toc-text">进一步阅读</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.5.</span> <span class="toc-text">构建神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E8%AE%AD%E7%BB%83%E8%AE%BE%E5%A4%87"><span class="toc-number">2.5.1.</span> <span class="toc-text">获取训练设备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E7%B1%BB"><span class="toc-number">2.5.2.</span> <span class="toc-text">定义类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%90%84%E5%B1%82%E8%AF%A6%E8%A7%A3"><span class="toc-number">2.5.3.</span> <span class="toc-text">模型的各层详解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-Flatten"><span class="toc-number">2.5.3.1.</span> <span class="toc-text">nn.Flatten</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-Linear"><span class="toc-number">2.5.3.2.</span> <span class="toc-text">nn.Linear</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-ReLU"><span class="toc-number">2.5.3.3.</span> <span class="toc-text">nn.ReLU</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-Sequential"><span class="toc-number">2.5.3.4.</span> <span class="toc-text">nn.Sequential</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-Softmax"><span class="toc-number">2.5.3.5.</span> <span class="toc-text">nn.Softmax</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">2.5.4.</span> <span class="toc-text">模型参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E9%98%85%E8%AF%BB-3"><span class="toc-number">2.5.5.</span> <span class="toc-text">进一步阅读</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-torch-autograd-%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-number">2.6.</span> <span class="toc-text">使用 torch.autograd 进行自动微分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E3%80%81%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">2.6.1.</span> <span class="toc-text">张量、函数与计算图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6"><span class="toc-number">2.6.2.</span> <span class="toc-text">计算梯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A6%81%E7%94%A8%E6%A2%AF%E5%BA%A6%E8%BF%BD%E8%B8%AA"><span class="toc-number">2.6.3.</span> <span class="toc-text">禁用梯度追踪</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9B%B4%E5%A4%9A%E5%85%B3%E4%BA%8E%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E5%86%85%E5%AE%B9"><span class="toc-number">2.6.4.</span> <span class="toc-text">更多关于计算图的内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E9%80%89%E9%98%85%E8%AF%BB%EF%BC%9A%E5%BC%A0%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8E%E9%9B%85%E5%8F%AF%E6%AF%94%E7%9F%A9%E9%98%B5%E4%B9%98%E7%A7%AF"><span class="toc-number">2.6.5.</span> <span class="toc-text">可选阅读：张量梯度与雅可比矩阵乘积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E9%98%85%E8%AF%BB-4"><span class="toc-number">2.6.6.</span> <span class="toc-text">进一步阅读</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0-2"><span class="toc-number">2.7.</span> <span class="toc-text">优化模型参数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E7%BD%AE%E4%BB%A3%E7%A0%81"><span class="toc-number">2.7.1.</span> <span class="toc-text">前置代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-number">2.7.2.</span> <span class="toc-text">超参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%BE%AA%E7%8E%AF"><span class="toc-number">2.7.3.</span> <span class="toc-text">优化循环</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88Loss-Function%EF%BC%89"><span class="toc-number">2.7.3.1.</span> <span class="toc-text">损失函数（Loss Function）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88Optimizer%EF%BC%89"><span class="toc-number">2.7.3.2.</span> <span class="toc-text">优化器（Optimizer）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.7.4.</span> <span class="toc-text">完整实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E9%98%85%E8%AF%BB-5"><span class="toc-number">2.7.5.</span> <span class="toc-text">进一步阅读</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%B9%B6%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.8.</span> <span class="toc-text">保存并加载模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D"><span class="toc-number">2.8.1.</span> <span class="toc-text">保存和加载模型权重</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E5%B8%A6%E6%9C%89%E7%BB%93%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.8.2.</span> <span class="toc-text">保存和加载带有结构的模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E6%95%99%E7%A8%8B"><span class="toc-number">2.8.3.</span> <span class="toc-text">相关教程</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PyTorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" title="学习 Pytorch 基础知识">学习 Pytorch 基础知识</a><time datetime="2025-05-24T16:33:10.260Z" title="发表于 2025-05-25 00:33:10">2025-05-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/" title="小波变换">小波变换</a><time datetime="2025-05-22T01:54:11.022Z" title="发表于 2025-05-22 09:54:11">2025-05-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/15/%E7%AE%97%E6%B3%95/%E5%9F%BA%E7%A1%80%E6%9A%B4%E5%8A%9B%E7%AE%97%E6%B3%95/" title="基础暴力算法">基础暴力算法</a><time datetime="2025-04-15T03:39:06.386Z" title="发表于 2025-04-15 11:39:06">2025-04-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/04/23/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CSAPP-1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%BC%AB%E6%B8%B8/" title="CSAPP-1计算机系统漫游">CSAPP-1计算机系统漫游</a><time datetime="2023-04-22T18:10:33.000Z" title="发表于 2023-04-23 02:10:33">2023-04-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/04/20/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/OSTEP-1%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B8%8A%E7%9A%84%E7%A8%8B%E5%BA%8F/" title="OSTEP-1操作系统上的程序">OSTEP-1操作系统上的程序</a><time datetime="2023-04-20T14:12:13.000Z" title="发表于 2023-04-20 22:12:13">2023-04-20</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By zyw9825</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div><div class="footer_custom_text"><img src="https://haiyong.site/img/icp.png"> <a href="https://beian.miit.gov.cn"  style="color:white" target="_blank">陕ICP备2025066278号-1</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><div class="aplayer no-destroy" data-lrctype="0" data-id="922753585" data-order="random" data-listmaxheight="200px" data-theme="#2980b9" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true"> </div><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script>(() => {
  const destroyAplayer = () => {
    if (window.aplayers) {
      for (let i = 0; i < window.aplayers.length; i++) {
        if (!window.aplayers[i].options.fixed) {
          window.aplayers[i].destroy()
        }
      }
    }
  }

  const runMetingJS = () => {
    typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()
  }

  btf.addGlobalFn('pjaxSend', destroyAplayer, 'destroyAplayer')
  btf.addGlobalFn('pjaxComplete', loadMeting, 'runMetingJS')
})()</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => fn())
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      false 
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>